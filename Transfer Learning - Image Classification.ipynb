{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Deep Learning Image Classifier Using Transfer Learning\n",
    "\n",
    "### Presented by: [Ivan Hernandez, Ph.d](\"http://ivanhernandez.com\"), Virginia Tech\n",
    "\n",
    "\n",
    "#### To run the code in each cell: \n",
    "- #### Click inside the cell\n",
    "- #### Hold down Ctrl and then press Enter\n",
    "- #### An Asterisk (*) should appear in the top left corner of the cell when the code is running\n",
    "- #### Output should appear when the code has successfully run\n",
    "- #### If you want to stop execution of the code, press the stop sign (â– ) at the top menu bar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the Data\n",
    "\n",
    "The first step of training a deep learning classifier is importing data.\n",
    "\n",
    "We assume the following:\n",
    "- Images are saved in a folder called \"images\" within the same directory as your code file.\n",
    "- Images within that folder are saved in different folders corresponding to their classification (e.g., \"female\", \"male\"\n",
    "- Images are saved at a size large than 224 x 224 pixels.\n",
    "- Images are saved as a jpeg or png\n",
    "\n",
    "We first import the required libraries to load the data including\n",
    "- torch (contains libraries to construct and train neural networks)\n",
    "- torchvision (contains libraries to load and tranform images into data for neural networks)\n",
    "\n",
    "\n",
    "We create a normalization function that standardizes the image color channels (red, green, blue) to have the same mean and standard deviation. The values provided are the ones most commonly used.\n",
    "\n",
    "We define a transformation function that allows images to have transformatons applied during training that promote generalizability including:\n",
    "- Randomly resizing the cropping the image (between 80% to 100% of the original size\n",
    "- Resizing the image to a common size of 224 x 224 pixels\n",
    "- Randomly deciding whether to flip the image horizontally\n",
    "- Converting the image to a numeric matrix (tensor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the required libraries\n",
    "import torchvision\n",
    "import torch\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "#create a function that defines the means and standard deviations that we constrain the image's red, green, and blue channels\n",
    "normalize = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "\n",
    "#Create a function that applies data transformation to the images used to the train the model\n",
    "transform_data = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "            torchvision.transforms.RandomHorizontalFlip(),\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ])\n",
    "\n",
    "#Load the images in the folder called \"images\".\n",
    "#The \"images\" folders should have subfolders with names that correspond to the different categories (e.g., \"male\" and \"female\")\n",
    "# Within the category subfolders there should be a college of images belonging to that category\n",
    "dataset = torchvision.datasets.ImageFolder('images',transform=transform_data)    \n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Break Data into Training and Validation and Test Sets\n",
    "\n",
    "Now that we have our data loaded, we need to split it into different sets for training, validation and testing.\n",
    "\n",
    "<img src = \"notebookfiles/train-validate-test.png\">\n",
    "\n",
    "The subsets of data are used in the following ways:\n",
    "- Training set: used to provide the models examples to learn the correct weight values to predict the images\n",
    "- Validation set: used to evaluate the performance of the training test to determine when to stop the training\n",
    "- Test set: used to evaluate the performance of the model and correct for overfitting that would other occur on the validation set due to using the validation set to configure the weights\n",
    "\n",
    "We need to specify our validation size (percentage of original data we set aside for testing).\n",
    "\n",
    "The validation size is a percentage. If we set it to 10%, then if we had 1000 total images, 100 of them would be used for the validation set and 100 of them would be used for the testing set.\n",
    "\n",
    "We also need to specify our batch size, which is how we group our images during training. With neural networks. we can send pass images through the network in batches rather than one-by-one.\n",
    "\n",
    "Batch sizes are almost always powers of 2 because of the effective work of optimized matrix operation libraries. Generally, the batch sizes of 32, 64, 128, 256, 512 and 1024 are used while training a neural network. \n",
    "\n",
    "Using a large batch size makes the neural network perform poorly. This lack of generalizability is due to the fact that large batch size tends to converge to a sharp minimizer of the training function. Small batch sizes tends to converge to flat minimizers, and also not produce the best results. Small batches though fit into memory better. \n",
    "\n",
    "You generally want to choose one that is moderately sized, but if you get a memory error, try using a smaller batch. We are using a batch size of 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the numpy library and abbreviate its name to np\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(1234)\n",
    "\n",
    "#Specify what percentage of the original set of images should be set aside for validation and final testing\n",
    "validation_size = .10\n",
    "\n",
    "#Specify how many images are used to train the model at once\n",
    "batch_size = 16\n",
    "\n",
    "\n",
    "num_train = len(dataset) #determine how many total images do we have\n",
    "indices = list(range(num_train)) # a list of indices, one for each instance\n",
    "validation_amount = int(np.floor(validation_size * num_train)) \n",
    "split = num_train - 2 * validation_amount #calculate the amount of images used to train the model\n",
    "np.random.shuffle(indices) # randomly shuffle the indices\n",
    "\n",
    "#split the indices into ones that correspond to the training images, validation images, and testing images.\n",
    "train_idx, valid_idx, test_idx = indices[:split], indices[split:split+validation_amount],indices[split+validation_amount:]\n",
    "\n",
    "#Create sampler objects that can randomly sample rows according to a list of all possible people\n",
    "train_sampler = torch.utils.data.sampler.SubsetRandomSampler(train_idx)\n",
    "valid_sampler = torch.utils.data.sampler.SubsetRandomSampler(valid_idx)\n",
    "test_sampler = torch.utils.data.sampler.SubsetRandomSampler(test_idx)\n",
    "\n",
    "\n",
    "#Create data loaders that are provided with the same dataset, but a different \n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "valid_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=valid_sampler)      \n",
    "test_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=test_sampler)\n",
    "\n",
    "print(\"Data Loaders Initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load a Pretrained Model\n",
    "\n",
    "For transfer learning, you need to load a pretrain model that has been trained on a problem as close to you current problem as possible.\n",
    "\n",
    "For image classification tasks, it is common to load models pretrained on the ImageNet task (a competition to classify the object of images).\n",
    "\n",
    "Although there are many possible neural network configurations to classify images (VGGNet, LeNet, INCEPTION), Resnet is very common to use.\n",
    "\n",
    "<img src=\"notebookfiles/resnetimage.png\">\n",
    "\n",
    "There are many different sized Resnet models to choose from. In general, larger networks can attain higher accuracy, but take longer to train, and may require more training examples.\n",
    "\n",
    "In Torchvision, some pretrained ResNet models are:\n",
    "- resnet18 (ResNet with 18 hidden layers)\n",
    "- resnet34 (ResNet with 34 hidden layers)\n",
    "- resnet50 (ResNet with 50 hidden layers)\n",
    "- resnet101 (ResNet with 101 hidden layers)\n",
    "- resnet152 (ResNet with 152 hidden layers)\n",
    "\n",
    "We will use resnet18, which has 18 hidden layers (the smallest option possible)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "#Load in a pretrained model that we have saved\n",
    "pretrained_model = torch.load(\"resnet18.pth\")\n",
    "\n",
    "#uncomment the line below to download new models from the internet\n",
    "#pretrained_model = models.resnet18(pretrained=True) \n",
    "\n",
    "print(pretrained_model) #View the model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Freeze the Pretrained Model's Weights\n",
    "\n",
    "When performing transfer learning, you need to tell the neural network not to change any of the weights from the earlier layers.\n",
    "\n",
    "This process is called \"freezing the weights\"\n",
    "\n",
    "<img src=\"notebookfiles/resnettransferlearning.png\">\n",
    "\n",
    "In the code below, we will freeze all of the weights in the model, and then later we will create new layers to replace the later ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in list(pretrained_model.children()):\n",
    "    param.require_grad = False\n",
    "\n",
    "print(\"All model weights are frozen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change the Classification Layer\n",
    "\n",
    "After freezing the weights to the model, we need to overwrite the existing layers at the end of the model that handle classification from the features inferred from the earlier layers.\n",
    "\n",
    "Below, we specify how many possible outcome categories we have (2), and then determine how many features were fed into the last layer (called the fully connected or fc layer).\n",
    "\n",
    "We then overwrite the fully connected layer with a new Linear neural network layer that accepts the same input as before, but outputs the desired number of categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_classes = 2 #indicate how many outcome categories there are\n",
    "\n",
    "num_ftrs = pretrained_model.fc.in_features #get the features produced by the second to last layer going into the last\n",
    "\n",
    "#Overwrite the last layer (fully connected / fc) with a new Linear model that takes in the same amount of information\n",
    "#The new layer outputs data equal to the number of possible outcome categories\n",
    "pretrained_model.fc = torch.nn.Linear(num_ftrs, number_classes)\n",
    "print(\"Last Layer Overwritten\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Model to Train on the Desired Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the code will use your GPU if you have it configured correctly\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
    "pretrained_model = pretrained_model.to(device) #have the model train on the preferred device\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the Model and Optimizers\n",
    "\n",
    "We define the loss function.\n",
    "\n",
    "For classifiction tasks, CrossEntropyLoss is commonly used.\n",
    "\n",
    "For regression tasks, MSELoss is commonly used.\n",
    "\n",
    "We also define what optimization algorithm we will use to determine how to update the weights of the model\n",
    "\n",
    "We are using Stochastic Gradient Descent, but Adam and AdaDelta are also popular.\n",
    "\n",
    "We then define the scheduler, which determines how much to slow down the learning rate (lr) after a specific number of losses.\n",
    "\n",
    "We are setting the learning rate to .01. If you wanted to make the learning rate slower, you would change it to .001. SLowing the learning rate takes more time, but often improves the prediction results (up to a limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose the Loss Function. For Classification Problems, CrossEntropyLoss is good\n",
    "loss_function = torch.nn.CrossEntropyLoss() \n",
    "\n",
    "#Define the optimization algorithm used and it's learning rate\n",
    "# Observe that only the fc parameters are being optimized\n",
    "optimizer = torch.optim.SGD(pretrained_model.fc.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 3 epochs\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Functions to Train and Test the model\n",
    "\n",
    "The following functions are used to train the model and examine its performance on a given dataset.\n",
    "\n",
    "The evaluation function is provided\n",
    "- the dataloader containing either training or validation data\n",
    "- the current epoch\n",
    "- a True or False flag indicating whether the function is being used to train or evaluate the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_loader,epoch,train=True):\n",
    "    losses = []\n",
    "    if train:\n",
    "        pretrained_model.train()\n",
    "        for batch_idx, (data, target) in enumerate(data_loader):\n",
    "            if torch.cuda.is_available(): \n",
    "                data, target = torch.autograd.Variable(data).cuda(), torch.autograd.Variable(target).cuda()\n",
    "            else: \n",
    "                data, target = torch.autograd.Variable(data), torch.autograd.Variable(target)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = pretrained_model(data)\n",
    "            loss = loss_function(output, target)\n",
    "            \n",
    "            print(\"Train Epoch: \",epoch+1, \"Batch :\", batch_idx, \"Loss: \", np.float(loss.data))\n",
    "            \n",
    "            #check the order of these\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "    else:\n",
    "        pretrained_model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (data, target) in enumerate(data_loader):\n",
    "                \n",
    "                if  torch.cuda.is_available(): \n",
    "                    data, target = torch.autograd.Variable(data).cuda(), torch.autograd.Variable(target).cuda()\n",
    "                else: \n",
    "                    data, target = torch.autograd.Variable(data), torch.autograd.Variable(target)\n",
    "\n",
    "                output = pretrained_model(data)\n",
    "                loss = loss_function(output, target)\n",
    "                losses.append(np.float(loss.data))\n",
    "    \n",
    "    return np.mean(losses)\n",
    "\n",
    "print(\"Training Function Defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model\n",
    "\n",
    "The following code is the key part that actually trains the model with the dataset and displays its current performance.\n",
    "\n",
    "Run this code ONLY when you have run the previous functions.\n",
    "\n",
    "The code below:\n",
    "- iterates through a given number of epochs (10 in this example)\n",
    "- passes the training data through the model (using the train_ loader)\n",
    "- updates the weights (occurs within the evaluate function)\n",
    "- evaluates the model on the validation dataset (using the test loader)\n",
    "- compares the loss on the validation data from that epoch to the best previously observed loss.\n",
    "- saves the current if it improves on the current loss\n",
    "\n",
    "<img src=\"notebookfiles/learningcurve.jpg\" height=300 width=300>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "number_of_epochs = 5 #How many passes through the data are you making\n",
    "\n",
    "\n",
    "best_loss =  float(\"inf\") #start out with the loss being the worst possible (inifinity)\n",
    "for epoch in range(number_of_epochs): #train the model by passing the data through it 5 separate times\n",
    "    train_loss = evaluate(train_loader,epoch,train=True) #train the model\n",
    "    test_loss = evaluate(valid_loader,epoch,train=False) #test the model on the validation data\n",
    "    if test_loss < best_loss: #if the validation loss has improved upon the previously observed validation loss\n",
    "        best_model = copy.deepcopy(pretrained_model) #copy the model as the new best model\n",
    "        best_loss = test_loss #update the best observed loss\n",
    "    scheduler.step() #increment the schedulers by one epoch\n",
    "    print(\"Epoch: \", epoch+1, \"Test Loss: \", test_loss) #print the validation results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View Final Evaluation\n",
    "\n",
    "When the model has bee trained, it is important to evaluate the model's performance on a separate dataset not used to select the best combination of model weights.\n",
    "\n",
    "The following function uses the model to make predictions on a the data in the test loader, and then compares those predictions to the actual outcome values, and calculates the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "correct_predictions = 0\n",
    "predictions_made = 0\n",
    "\n",
    "pretrained_model.eval()\n",
    "\n",
    "for batch_idx, (data, target) in enumerate(test_loader):\n",
    "    with torch.no_grad():\n",
    "        if torch.cuda.is_available(): \n",
    "            data, target = torch.autograd.Variable(data).cuda(), torch.autograd.Variable(target).cuda()\n",
    "\n",
    "        else: \n",
    "            data, target = torch.autograd.Variable(data), torch.autograd.Variable(target)\n",
    "\n",
    "        output = best_model(data)\n",
    "\n",
    "        predicted = output.data.max(1)[1]\n",
    "        correct_predictions += predicted.eq(target.data).cpu().sum()\n",
    "        predictions_made += len(target.data)\n",
    "\n",
    "print(\"Test Accuracy :\", float(correct_predictions) / predictions_made)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the Model\n",
    "\n",
    "When you are satisfied with your model, you can save it to the directory where the notebook file is running.\n",
    "\n",
    "It is common to use the extension \".pth\" for pytorch models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best_model, \"updated_model.pth\") #you can call the filename whatever you want. We use \"updated_model.pth\" here\n",
    "print(\"model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Model\n",
    "\n",
    "You can load previously saved models and assign them to a variable name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = torch.load(\"updated_model.pth\")\n",
    "print(\"model loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a Prediction\n",
    "\n",
    "You can use the model to make predictions on a new image using the following code.\n",
    "\n",
    "The predicted value is a number.\n",
    "\n",
    "If the predicted value is 0, it means the neural network is predicting the 1st category alphabetically (\"female\") from the folder names.\n",
    "\n",
    "A prediction of 1 represents the next alphabetic category (\"male\") that was in the folder names.\n",
    "\n",
    "If there was another folder name (\"other\") whose alphabetic order was after category 1, it would be prediction 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "image_filename = \"whitehouse/hilary.jpg\"\n",
    "#try the names of other images saved in the folder\n",
    "# \"bill.jpg\"\n",
    "# \"barack.jpg\"\n",
    "# \"michelle.jpg\"\n",
    "# \"melania.jpg\"\n",
    "# \"donald.jpg\"\n",
    "\n",
    "\n",
    "pretrained_model.eval()\n",
    "\n",
    "img = Image.open(image_filename)\n",
    "\n",
    "#If you wanted to use a url, you can use the following code (uncomment the next three lines):\n",
    "#url = \"https://upload.wikimedia.org/wikipedia/commons/f/f5/Poster-sized_portrait_of_Barack_Obama.jpg\"\n",
    "#response = requests.get(url)\n",
    "#img = Image.open(BytesIO(response.content))\n",
    "\n",
    "normalize = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "\n",
    "preprocess = torchvision.transforms.Compose([\n",
    "   torchvision.transforms.Resize ((224,224)),\n",
    "   torchvision.transforms.ToTensor(),\n",
    "   normalize\n",
    "])\n",
    "\n",
    "img_tensor = preprocess(img)\n",
    "img_tensor.unsqueeze_(0)\n",
    "\n",
    "if torch.cuda.is_available(): \n",
    "    img_variable = torch.autograd.Variable(img_tensor).cuda()\n",
    "else:\n",
    "    img_variable = torch.autograd.Variable(img_tensor)\n",
    "    \n",
    "output = pretrained_model(img_variable)\n",
    "predicted = torch.max(output.data,1)[1].item()\n",
    "\n",
    "print(\"Predicted category: \", predicted)\n",
    "img.resize((200, int(200 * (img.height / img.width))),resample= Image.ANTIALIAS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
